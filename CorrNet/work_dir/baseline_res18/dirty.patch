diff --git a/configs/baseline.yaml b/configs/baseline.yaml
index 1252471..13cbdba 100644
--- a/configs/baseline.yaml
+++ b/configs/baseline.yaml
@@ -7,13 +7,13 @@ work_dir: ./work_dir/baseline_res18/
 batch_size: 2
 random_seed: 0
 test_batch_size: 2
-num_worker: 10
-device: 0,1
+num_worker: 0
+device: cpu
 log_interval: 10000
 eval_interval: 1
 save_interval: 5
 # python in default
-evaluate_tool: sclite
+evaluate_tool: python
 loss_weights:
   SeqCTC: 1.0
   # VAC
diff --git a/dataset/dataloader_video.py b/dataset/dataloader_video.py
index ca34311..14b5b9a 100644
--- a/dataset/dataloader_video.py
+++ b/dataset/dataloader_video.py
@@ -13,7 +13,6 @@ import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 
 import numpy as np
-# import pyarrow as pa
 from PIL import Image
 import torch.utils.data as data
 import matplotlib.pyplot as plt
@@ -21,7 +20,6 @@ from utils import video_augmentation
 from torch.utils.data.sampler import Sampler
 
 sys.path.append("..")
-global kernel_sizes 
 
 class BaseFeeder(data.Dataset):
     def __init__(self, prefix, gloss_dict, dataset='phoenix2014', drop_ratio=1, num_gloss=-1, mode="train", transform_mode=True,
@@ -33,10 +31,9 @@ class BaseFeeder(data.Dataset):
         self.data_type = datatype
         self.dataset = dataset
         self.input_size = input_size
-        global kernel_sizes 
-        kernel_sizes = kernel_size
-        self.frame_interval = frame_interval # not implemented for read_features()
-        self.image_scale = image_scale # not implemented for read_features()
+        self.kernel_sizes = [kernel_size] if isinstance(kernel_size, int) else kernel_size
+        self.frame_interval = frame_interval
+        self.image_scale = image_scale
         self.feat_prefix = f"{prefix}/features/fullFrame-256x256px/{mode}"
         self.transform_mode = "train" if transform_mode else "test"
         self.inputs_list = np.load(f"./preprocess/{dataset}/{mode}_info.npy", allow_pickle=True).item()
@@ -48,7 +45,6 @@ class BaseFeeder(data.Dataset):
         if self.data_type == "video":
             input_data, label, fi = self.read_video(idx)
             input_data, label = self.normalize(input_data, label)
-            # input_data, label = self.normalize(input_data, label, fi['fileid'])
             return input_data, torch.LongTensor(label), self.inputs_list[idx]['original_info']
         elif self.data_type == "lmdb":
             input_data, label, fi = self.read_lmdb(idx)
@@ -78,7 +74,6 @@ class BaseFeeder(data.Dataset):
         return [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list], label_list, fi
 
     def read_features(self, index):
-        # load file info
         fi = self.inputs_list[index]
         data = np.load(f"./features/{self.mode}/{fi['fileid']}_features.npy", allow_pickle=True).item()
         return data['features'], data['label']
@@ -92,8 +87,6 @@ class BaseFeeder(data.Dataset):
         if self.transform_mode == "train":
             print("Apply training transform.")
             return video_augmentation.Compose([
-                # video_augmentation.CenterCrop(224),
-                # video_augmentation.WERAugment('/lustre/wangtao/current_exp/exp/baseline/boundary.npy'),
                 video_augmentation.RandomCrop(self.input_size),
                 video_augmentation.RandomHorizontalFlip(0.5),
                 video_augmentation.Resize(self.image_scale),
@@ -108,64 +101,38 @@ class BaseFeeder(data.Dataset):
                 video_augmentation.ToTensor(),
             ])
 
-    def byte_to_img(self, byteflow):
-        unpacked = pa.deserialize(byteflow)
-        imgbuf = unpacked[0]
-        buf = six.BytesIO()
-        buf.write(imgbuf)
-        buf.seek(0)
-        img = Image.open(buf).convert('RGB')
-        return img
-
     @staticmethod
-    def collate_fn(batch):
-        batch = [item for item in sorted(batch, key=lambda x: len(x[0]), reverse=True)]
-        video, label, info = list(zip(*batch))
-        
-        left_pad = 0
-        last_stride = 1
-        total_stride = 1
-        global kernel_sizes 
-        for layer_idx, ks in enumerate(kernel_sizes):
+    def collate_fn(batch):  # ⚡ Jetzt statische Methode
+        batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)
+        video, label, info = zip(*batch)
+
+        left_pad, last_stride, total_stride = 0, 1, 1
+        kernel_sizes = [[3], [2]]  # Falls `kernel_sizes` nicht vorhanden ist, Standardwerte nutzen
+        for ks in kernel_sizes:
             if ks[0] == 'K':
-                left_pad = left_pad * last_stride 
-                left_pad += int((int(ks[1])-1)/2)
+                left_pad = left_pad * last_stride + int((int(ks[1]) - 1) / 2)
             elif ks[0] == 'P':
                 last_stride = int(ks[1])
-                total_stride = total_stride * last_stride
+                total_stride *= last_stride
+
         if len(video[0].shape) > 3:
             max_len = len(video[0])
-            video_length = torch.LongTensor([np.ceil(len(vid) / total_stride) * total_stride + 2*left_pad for vid in video])
+            video_length = torch.LongTensor([np.ceil(len(vid) / total_stride) * total_stride + 2 * left_pad for vid in video])
             right_pad = int(np.ceil(max_len / total_stride)) * total_stride - max_len + left_pad
-            max_len = max_len + left_pad + right_pad
-            padded_video = [torch.cat(
-                (
-                    vid[0][None].expand(left_pad, -1, -1, -1),
-                    vid,
-                    vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),
-                )
-                , dim=0)
-                for vid in video]
+            max_len += left_pad + right_pad
+            padded_video = [torch.cat([vid[0][None].expand(left_pad, -1, -1, -1), vid, vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1)], dim=0) for vid in video]
             padded_video = torch.stack(padded_video)
         else:
             max_len = len(video[0])
             video_length = torch.LongTensor([len(vid) for vid in video])
-            padded_video = [torch.cat(
-                (
-                    vid,
-                    vid[-1][None].expand(max_len - len(vid), -1),
-                )
-                , dim=0)
-                for vid in video]
+            padded_video = [torch.cat([vid, vid[-1][None].expand(max_len - len(vid), -1)], dim=0) for vid in video]
             padded_video = torch.stack(padded_video).permute(0, 2, 1)
+
         label_length = torch.LongTensor([len(lab) for lab in label])
         if max(label_length) == 0:
             return padded_video, video_length, [], [], info
         else:
-            padded_label = []
-            for lab in label:
-                padded_label.extend(lab)
-            padded_label = torch.LongTensor(padded_label)
+            padded_label = torch.LongTensor([lab for sublist in label for lab in sublist])
             return padded_video, video_length, padded_label, label_length, info
 
     def __len__(self):
@@ -182,13 +149,17 @@ class BaseFeeder(data.Dataset):
 
 
 if __name__ == "__main__":
-    feeder = BaseFeeder()
+    feeder = BaseFeeder(prefix="./data", gloss_dict={})  # Beispielhafte Parameter
     dataloader = torch.utils.data.DataLoader(
-        dataset=feeder,
-        batch_size=1,
-        shuffle=True,
-        drop_last=True,
-        num_workers=0,
-    )
+    dataset=feeder,
+    batch_size=1,
+    shuffle=True,
+    drop_last=True,
+    num_workers=0,
+    collate_fn=BaseFeeder.collate_fn  # ⚡ Statische Methode aufrufen
+)
+
+
     for data in dataloader:
         pdb.set_trace()
+
diff --git a/main.py b/main.py
index 51b2e07..bf2871a 100644
--- a/main.py
+++ b/main.py
@@ -49,7 +49,11 @@ class Processor():
         self.data_loader = {}
         self.gloss_dict = np.load(self.arg.dataset_info['dict_path'], allow_pickle=True).item()
         self.arg.model_args['num_classes'] = len(self.gloss_dict) + 1
+        print(f"🛠 Model num_classes: {self.arg.model_args['num_classes']}, Gloss Dict Size: {len(self.gloss_dict)}")
         self.model, self.optimizer = self.loading()
+        print(f"📜 Original gloss_dict: {self.gloss_dict}")
+        print(f"🔢 Anzahl der Glossen: {len(self.gloss_dict)}")
+
 
     def start(self):
         if self.arg.phase == 'train':
@@ -122,6 +126,7 @@ class Processor():
             'rng_state': self.rng.save_rng_state(),
         }, save_path)
 
+
     def loading(self):
         self.device.set_device(self.arg.device)
         print("Loading model")
@@ -143,18 +148,23 @@ class Processor():
         print("Loading model finished.")
         self.load_data()
         return model, optimizer
+    
 
     def model_to_device(self, model):
-        model = model.to(self.device.output_device)
-        if len(self.device.gpu_list) > 1:
-            raise ValueError("AMP equipped with DataParallel has to manually write autocast() for each forward function, you can choose to do this by yourself")
-            #model.conv2d = nn.DataParallel(model.conv2d, device_ids=self.device.gpu_list, output_device=self.device.output_device)
-        model = convert_model(model)
-        model.cuda()
+        if self.device.output_device == "cpu":
+            model = model.to("cpu")  # Modell auf CPU verschieben
+        else:
+            model = model.to(self.device.output_device)  # Modell auf GPU verschieben
+            if len(self.device.gpu_list) > 1:
+                model = nn.DataParallel(
+                    model,
+                    device_ids=self.device.gpu_list,
+                    output_device=self.device.output_device
+                )
         return model
 
     def load_model_weights(self, model, weight_path):
-        state_dict = torch.load(weight_path)
+        state_dict = torch.load(weight_path, map_location=torch.device('cpu'))
         if len(self.arg.ignore_weights):
             for w in self.arg.ignore_weights:
                 if state_dict.pop(w, None) is not None:
@@ -234,7 +244,7 @@ def import_class(name):
 if __name__ == '__main__':
     sparser = utils.get_parser()
     p = sparser.parse_args()
-    # p.config = "baseline_iter.yaml"
+    #p.config = "baseline_iter.yaml"
     if p.config is not None:
         with open(p.config, 'r') as f:
             try:
diff --git a/requirements.txt b/requirements.txt
index 5a225cd..c33fea0 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,9 +1,9 @@
-matplotlib==3.4.3
-numpy==1.20.3
-opencv_python==4.5.5.64
-pandas==1.3.4
-Pillow==9.4.0
-PyYAML==6.0
-scipy==1.7.1
-six==1.16.0
-tqdm==4.62.3
+matplotlib
+numpy
+opencv_python
+pandas
+Pillow
+PyYAML
+scipy
+six
+tqdm
diff --git a/utils/decode.py b/utils/decode.py
index f0c27d9..f18b3fa 100644
--- a/utils/decode.py
+++ b/utils/decode.py
@@ -2,22 +2,47 @@ import os
 import pdb
 import time
 import torch
-import ctcdecode
 import numpy as np
 from itertools import groupby
 import torch.nn.functional as F
-
+from torchaudio.models.decoder import ctc_decoder
 
 class Decode(object):
     def __init__(self, gloss_dict, num_classes, search_mode, blank_id=0):
+        print(f"📜 Inhalt von gloss_dict: {gloss_dict}")
+
         self.i2g_dict = dict((v[0], k) for k, v in gloss_dict.items())
         self.g2i_dict = {v: k for k, v in self.i2g_dict.items()}
+
+        # 🛠 Debugging, um sicherzustellen, dass die Werte jetzt korrekt sind
+        print("✅ Debug: self.i2g_dict:", self.i2g_dict)  
+        print("✅ Debug: self.g2i_dict:", self.g2i_dict)
+
         self.num_classes = num_classes
         self.search_mode = search_mode
         self.blank_id = blank_id
-        vocab = [chr(x) for x in range(20000, 20000 + num_classes)]
-        self.ctc_decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=10, blank_id=blank_id,
-                                                    num_processes=10)
+        
+        self.vocab =  ['|']+['-']+[chr(x) for x in range(20000, 20000 + num_classes)]
+
+        # ✅ **Debugging: Überprüfe num_classes und Vokabular**
+        #print(f"✅ Baseline num_classes: {self.num_classes}, Vokabulargröße: {len(self.vocab)}")
+        #assert self.num_classes == len(self.vocab), "⚠️ Vokabulargröße passt nicht zu num_classes!"
+
+        #print(f"✅ Blank ID: {self.blank_id}")
+        #assert self.blank_id == 0, "⚠️ Blank ID ist nicht 0! Torchaudio erwartet meistens 0."
+
+        print(f"✅ Tokens im CTC-Decoder: {self.vocab}")
+        
+        self.ctc_decoder = ctc_decoder(
+            lexicon = None,
+            tokens=self.vocab,
+            beam_size=3
+        )
+        print(f"🔍 Debug: Verwende {self.ctc_decoder.__class__.__name__}")
+
+        # 🚀 **Überprüfe das finale Vokabular in PyCTCDecode**
+        print(f"✅ Torchaudio CTCDecoder verwendet: {self.vocab} (Größe: {len(self.vocab)})")
+
 
     def decode(self, nn_output, vid_lgt, batch_first=True, probs=False):
         if not batch_first:
@@ -26,31 +51,64 @@ class Decode(object):
             return self.MaxDecode(nn_output, vid_lgt)
         else:
             return self.BeamSearch(nn_output, vid_lgt, probs)
-
+        
     def BeamSearch(self, nn_output, vid_lgt, probs=False):
-        '''
-        CTCBeamDecoder Shape:
-                - Input:  nn_output (B, T, N), which should be passed through a softmax layer
-                - Output: beam_resuls (B, N_beams, T), int, need to be decoded by i2g_dict
-                          beam_scores (B, N_beams), p=1/np.exp(beam_score)
-                          timesteps (B, N_beams)
-                          out_lens (B, N_beams)
-        '''
         if not probs:
             nn_output = nn_output.softmax(-1).cpu()
         vid_lgt = vid_lgt.cpu()
-        beam_result, beam_scores, timesteps, out_seq_len = self.ctc_decoder.decode(nn_output, vid_lgt)
+        decoder_outputs= self.ctc_decoder(nn_output, vid_lgt)
+
+        # 🔍 Debugging: Zeige die rohe Decodierung an
+        print(f"🔍 Rohe Decodierung aus BeamSearch: {decoder_outputs}")
+
+        tokens_per_batch = [[hyp.tokens for hyp in batch] for batch in decoder_outputs]# these are the TOP results
+        timesteps_per_batch = [[hyp.timesteps for hyp in batch] for batch in decoder_outputs]
+
+        # ✅ Debugging: Zeige Tokens pro Batch
+        print(f"🔍 Tokens per Batch: {tokens_per_batch}")
+
+        # 🔍 1️⃣ Zeige die maximalen Wahrscheinlichkeiten pro Frame
+        max_probs, max_indices = nn_output.max(dim=-1)
+        print(f"🔍 Max-Wahrscheinlichkeiten pro Frame:\n{max_probs.numpy()}")
+        print(f"🔍 Höchstwahrscheinlich gewählte IDs:\n{max_indices.numpy()}")
+
+        #score_per_batch = [[hyp.score for hyp in batch] for batch in beam_outputs]
+        #words_per_batch = [[hyp.words for hyp in batch] for batch in beam_outputs]
+        #print(f' beam results: {tokens_per_batch}')
+        #print(f' timesteps: {timesteps_per_batch}')
+        #print(f' beam scores: {score_per_batch}')
+        #print(f' words: {words_per_batch}')
+
+        # 🔍 2️⃣ Zeige rohe BeamSearch-Tokens
+        print(f"🔍 Rohe BeamSearch-Tokens: {tokens_per_batch}")
+
+        # 🔍 3️⃣ Zeige Timesteps der ausgewählten Tokens
+        print(f"🔍 Timesteps per Batch: {timesteps_per_batch}")
+
         ret_list = []
-        for batch_idx in range(len(nn_output)):
-            first_result = beam_result[batch_idx][0][:out_seq_len[batch_idx][0]]
+        for batch_idx in range(len(tokens_per_batch)):  # Iterate over batches
+            first_result = tokens_per_batch[batch_idx][0][:len(timesteps_per_batch[batch_idx][0])]
+
+            print(f"🔍 first_result vor groupby: {first_result}, Typ: {type(first_result)}")
+            print(f"🔍 Tokens vor GroupBy: {first_result.tolist()}")
+            print(f"🔍 Mapped Glossen: {[self.i2g_dict.get(int(gloss_id), f'UNK({gloss_id})') for gloss_id in first_result.tolist()]}")
+
+
             if len(first_result) != 0:
                 first_result = torch.stack([x[0] for x in groupby(first_result)])
-            ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
-                             enumerate(first_result)])
+            
+            ret_list.append([
+                (self.i2g_dict.get(int(gloss_id), f"UNK({gloss_id})"), idx)
+                for idx, gloss_id in enumerate(first_result)
+            ])
+        
+        print(f"✅ Decoded sequence: {ret_list}")
         return ret_list
+    
 
     def MaxDecode(self, nn_output, vid_lgt):
         index_list = torch.argmax(nn_output, axis=2)
+        print(f'index_list.shape: {index_list.shape}')
         batchsize, lgt = index_list.shape
         ret_list = []
         for batch_idx in range(batchsize):
@@ -64,3 +122,4 @@ class Decode(object):
             ret_list.append([(self.i2g_dict[int(gloss_id)], idx) for idx, gloss_id in
                              enumerate(max_result)])
         return ret_list
+
diff --git a/utils/device.py b/utils/device.py
index 14d7cca..822cced 100644
--- a/utils/device.py
+++ b/utils/device.py
@@ -1,5 +1,4 @@
 import os
-import pdb
 import torch
 import torch.nn as nn
 
@@ -11,12 +10,15 @@ class GpuDataParallel(object):
 
     def set_device(self, device):
         device = str(device)
-        if device != 'None':
+        if device.lower() == 'cpu':
+            print("Using CPU")
+            self.gpu_list = []
+            self.output_device = torch.device('cpu')
+        else:
             self.gpu_list = [i for i in range(len(device.split(',')))]
             os.environ["CUDA_VISIBLE_DEVICES"] = device
-            output_device = self.gpu_list[0]
+            self.output_device = torch.device(f'cuda:{self.gpu_list[0]}')
             self.occupy_gpu(self.gpu_list)
-        self.output_device = output_device if len(self.gpu_list) > 0 else "cpu"
 
     def model_to_device(self, model):
         # model = convert_model(model)
@@ -49,9 +51,11 @@ class GpuDataParallel(object):
         """
             make program appear on nvidia-smi.
         """
-        if len(gpus) == 0:
-            torch.zeros(1).cuda()
+        if gpus is None or len(gpus) == 0:
+            print("No GPUs specified or using CPU")
         else:
             gpus = [gpus] if isinstance(gpus, int) else list(gpus)
             for g in gpus:
-                torch.zeros(1).cuda(g)
+                if torch.cuda.is_available():
+                    torch.zeros(1).cuda(g)
+
